% In typical experimentation paradigms, continual reallocation of measurement
% effort is expensive due to infrastructural and organizational difficulties,
% and is sometimes infeasible due to delayed feedback. Challenges in
% reallocation lead practitioners to employ a handful of reallocation epochs in
% which outcomes are measured in large batches. However, standard adaptive
% methods are specifically designed to do well when the number of reallocation
% epochs grows large. We develop a new adaptive experimentation framework that
% can flexibly handle any batch size and learns near-optimal designs when
% reallocation opportunities are few. By deriving an asymptotic sequential
% experiment based on normal approximations, we formulate a Bayesian dynamic
% program that can leverage prior information based on previous experiments. We
% propose an iterative method, $\algofull$, and demonstrate that despite relying
% on approximations it significantly improves statistical power over standard
% adaptive policies. Overall, our work shows normal approximations that are
% universal in statistical inference can also serve as powerful tools for
% adaptive experimental design.


Standard bandit algorithms that assume continual reallocation of measurement
effort are challenging to implement due to delayed feedback and
infrastructural/organizational difficulties. Motivated by practical instances
involving a handful of reallocation opportunities in which outcomes are
measured in batches, we develop a computation-driven adaptive experimentation
framework that can flexibly handle batching.  Our main observation is that
normal approximations, which are universal in statistical inference, can also
guide the design of adaptive algorithms. By deriving a Gaussian sequential
experiment, we formulate a dynamic program that can leverage prior information
on \emph{average rewards}.  Instead of the typical theory-driven paradigm, we
leverage computational tools and empirical benchmarking for algorithm
development. Our empirical analysis highlights a simple yet effective
algorithm, $\algofull$, which iteratively solves a planning problem using
stochastic gradient descent.  Our approach significantly improves power over
standard methods, even when compared to Bayesian algorithms (e.g., Thompson
sampling) that require full distributional knowledge of \emph{individual
rewards}.  Overall, we expand the scope of adaptive experimentation to
settings standard methods struggle with, involving few reallocation epochs,
low signal-to-noise ratio, and unknown reward distributions.


%%% TeX-master: "algorithms"
%%% Local Variables: %%% mode: latex %%% TeX-master: "main" %%% End:
