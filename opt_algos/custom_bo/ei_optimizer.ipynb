{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import benchmarks\n",
    "import optimizers\n",
    "import torch\n",
    "\n",
    "import matplotlib.pyplot as plt\n",
    "import numpy as np\n",
    "import gpytorch"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Instantiating benchmark with y=metric Wikipedia Cross Entropy\n"
     ]
    }
   ],
   "source": [
    "data_benchmark = benchmarks.DataModelBenchmark(metric_index=3)\n",
    "func = data_benchmark._raw_func_with_model_scale # (z, m, x)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_random_points(k):\n",
    "    rd_prop = np.random.dirichlet(np.ones(5), size=k)\n",
    "    rd_scale = np.random.choice([2, 15], size=k)\n",
    "    rd_timestep = np.random.choice(np.arange(1, 197), size=k)\n",
    "    rd_x = np.concatenate([rd_prop, rd_scale[:, None], rd_timestep[:, None]], axis=1)\n",
    "    return rd_x\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/var/folders/t3/ps6yd2g900v_jnbbvn4cq_wc0000gn/T/ipykernel_27150/888201946.py:16: UserWarning: Creating a tensor from a list of numpy.ndarrays is extremely slow. Please consider converting the list to a single numpy.ndarray with numpy.array() before converting to a tensor. (Triggered internally at /Users/runner/work/pytorch/pytorch/pytorch/torch/csrc/utils/tensor_new.cpp:278.)\n",
      "  train_x = torch.tensor(train_x)\n"
     ]
    }
   ],
   "source": [
    "# sample k random points from scale 2 and scale 15\n",
    "k = 10\n",
    "\n",
    "# x_shape: [5 categories, 1 scale, 1 timestep]\n",
    "rd_prop = np.random.dirichlet(np.ones(5), size=k)\n",
    "rd_scale = np.random.choice([2, 15], size=k)\n",
    "rd_timestep = np.random.choice(np.arange(1, 197), size=k)\n",
    "\n",
    "train_x = []\n",
    "train_y = []\n",
    "for i in range(k):\n",
    "    for s in range(1, rd_timestep[i] + 1):\n",
    "        train_x.append(np.concatenate([rd_prop[i], [rd_scale[i], s]]))\n",
    "        train_y.append(func(s, rd_scale[i], rd_prop[i]))\n",
    "\n",
    "train_x = torch.tensor(train_x)\n",
    "train_y = torch.tensor(train_y)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "lengthscale: 0.6931471824645996\n",
      "mean: 0.0\n",
      "Iter 1/50 - Loss: 0.7713402651153743\n",
      "Iter 2/50 - Loss: 0.581669325560806\n",
      "Iter 3/50 - Loss: 0.3926415176653024\n",
      "Iter 4/50 - Loss: 0.26682625379292263\n",
      "Iter 5/50 - Loss: 0.13959355535759105\n",
      "Iter 6/50 - Loss: 0.015600876507603448\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/michi/miniconda3/envs/dr/lib/python3.9/site-packages/gpytorch/likelihoods/gaussian_likelihood.py:347: GPInputWarning: You have passed data through a FixedNoiseGaussianLikelihood that did not match the size of the fixed noise, *and* you did not specify noise. This is treated as a no-op.\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Iter 7/50 - Loss: -0.12080163348815856\n",
      "Iter 8/50 - Loss: -0.2774048277440356\n",
      "Iter 9/50 - Loss: -0.36915128804477465\n",
      "Iter 10/50 - Loss: -0.5175718273593863\n",
      "Iter 11/50 - Loss: -0.6032017767305119\n",
      "Iter 12/50 - Loss: -0.7254832337767504\n",
      "Iter 13/50 - Loss: -0.855117562762108\n",
      "Iter 14/50 - Loss: -0.970593663343508\n",
      "Iter 15/50 - Loss: -1.0906515175297968\n",
      "Iter 16/50 - Loss: -1.1386108075972488\n",
      "Iter 17/50 - Loss: -1.173632932083773\n",
      "Iter 18/50 - Loss: -1.1429543300388234\n",
      "Iter 19/50 - Loss: -1.1535128738171572\n",
      "Iter 20/50 - Loss: -0.9879633367158034\n",
      "Iter 21/50 - Loss: -0.9011597700587985\n",
      "Iter 22/50 - Loss: -0.806759350342691\n",
      "Iter 23/50 - Loss: -0.7766735974103187\n",
      "Iter 24/50 - Loss: -0.8493836479621969\n",
      "Iter 25/50 - Loss: -0.9557860749419838\n",
      "Iter 26/50 - Loss: -1.0679850247563418\n",
      "Iter 27/50 - Loss: -1.139211012340212\n",
      "Iter 28/50 - Loss: -1.1937439444525433\n",
      "Iter 29/50 - Loss: -1.1903507949407277\n",
      "Iter 30/50 - Loss: -1.1654235053928725\n",
      "Iter 31/50 - Loss: -1.2217320044624076\n",
      "Iter 32/50 - Loss: -1.2491094641263154\n",
      "Iter 33/50 - Loss: -1.268917692531607\n",
      "Iter 34/50 - Loss: -1.2891363361886183\n",
      "Iter 35/50 - Loss: -1.229951145524081\n",
      "Iter 36/50 - Loss: -1.2948848163845335\n",
      "Iter 37/50 - Loss: -1.2906688655733491\n",
      "Iter 38/50 - Loss: -1.2877487270141257\n",
      "Iter 39/50 - Loss: -1.310141335181157\n",
      "Iter 40/50 - Loss: -1.2953843258778226\n",
      "Iter 41/50 - Loss: -1.2754348812304819\n",
      "Iter 42/50 - Loss: -1.2339762829509537\n",
      "Iter 43/50 - Loss: -1.2988949091899278\n",
      "Iter 44/50 - Loss: -1.2661336107923935\n",
      "Iter 45/50 - Loss: -1.242415911321283\n",
      "Iter 46/50 - Loss: -1.2128884607686061\n",
      "Iter 47/50 - Loss: -1.2541764956260635\n",
      "Iter 48/50 - Loss: -1.2519864567873427\n",
      "Iter 49/50 - Loss: -1.272068681318459\n",
      "Iter 50/50 - Loss: -1.2887819388910362\n",
      "Final lengthscale: 1.8431382179260254\n",
      "Final mean: 1.8270398378372192\n"
     ]
    }
   ],
   "source": [
    "import warnings\n",
    "from linear_operator.utils.warnings import NumericalWarning\n",
    "from collections import defaultdict\n",
    "warnings.filterwarnings('ignore', category=NumericalWarning)\n",
    "\n",
    "# Train initial GP\n",
    "import gpytorch\n",
    "class ExactGPModel(gpytorch.models.ExactGP):\n",
    "    def __init__(self, train_x, train_y, likelihood):\n",
    "        super(ExactGPModel, self).__init__(train_x, train_y, likelihood)\n",
    "        self.mean_module = gpytorch.means.ConstantMean()\n",
    "        self.covar_module = gpytorch.kernels.ScaleKernel(gpytorch.kernels.RBFKernel())\n",
    "\n",
    "    def forward(self, x):\n",
    "        mean_x = self.mean_module(x)\n",
    "        covar_x = self.covar_module(x)\n",
    "        return gpytorch.distributions.MultivariateNormal(mean_x, covar_x)\n",
    "\n",
    "likelihood = gpytorch.likelihoods.FixedNoiseGaussianLikelihood(noise=torch.ones(k) * 0.00)\n",
    "model = ExactGPModel(train_x, train_y, likelihood)\n",
    "print(f\"lengthscale: {model.covar_module.base_kernel.lengthscale.item()}\")\n",
    "print(f\"mean: {model.mean_module.constant.item()}\")\n",
    "\n",
    "training_iter = 50\n",
    "\n",
    "# Find optimal model hyperparameters\n",
    "model.train()\n",
    "\n",
    "# Use the adam optimizer\n",
    "optimizer = torch.optim.Adam(model.parameters(), lr=0.1)  # Includes GaussianLikelihood parameters\n",
    "\n",
    "# \"Loss\" for GPs - the marginal log likelihood\n",
    "mll = gpytorch.mlls.ExactMarginalLogLikelihood(likelihood, model)\n",
    "\n",
    "losses = []\n",
    "for i in range(training_iter):\n",
    "    # Zero gradients from previous iteration\n",
    "    optimizer.zero_grad()\n",
    "    # Output from model\n",
    "    output = model(train_x)\n",
    "    # Calc loss and backprop gradients\n",
    "    loss = -mll(output, train_y)\n",
    "    loss.backward()\n",
    "    losses.append(loss.item())\n",
    "    optimizer.step()\n",
    "    print(f\"Iter {i+1}/{training_iter} - Loss: {loss.item()}\")\n",
    "\n",
    "print(f\"Final lengthscale: {model.covar_module.base_kernel.lengthscale.item()}\")\n",
    "print(f\"Final mean: {model.mean_module.constant.item()}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Final lengthscale: tensor([[1.8431]], grad_fn=<SoftplusBackward0>)\n",
      "Final noise: tensor([[1.8431]], grad_fn=<SoftplusBackward0>)\n"
     ]
    }
   ],
   "source": [
    "print(f\"Final lengthscale: {model.covar_module.base_kernel.lengthscale}\")\n",
    "print(f\"Final noise: {model.covar_module.base_kernel.lengthscale}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{100: 1, 2: 0.012963288309111339, 6: 0.032318410379188076, 15: 0.07484392986066876, 30: 0.147730659731539, 50: 0.21662249349870538, 70: 0.29829799480020447}\n"
     ]
    }
   ],
   "source": [
    "flops = {\n",
    "    100: 1,\n",
    "    2: 2090524455 / 161264981936,\n",
    "    6: 5211827866 / 161264981936,\n",
    "    15: 12069704997 / 161264981936,\n",
    "    30: 23823782173 / 161264981936,\n",
    "    50: 34933622501 / 161264981936,\n",
    "    70: 48105020743 / 161264981936,\n",
    "}\n",
    "print(flops)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Cost of 20M runs: 8.633550013868152\n",
      "Cost of 150M runs: 25.746311872070056\n"
     ]
    }
   ],
   "source": [
    "# num of steps in 20M\n",
    "c2 = np.sum(np.sum(rd_timestep[rd_scale == 2])) * flops[2]\n",
    "c15 = np.sum(np.sum(rd_timestep[rd_scale == 15])) * flops[15]\n",
    "\n",
    "print(f\"Cost of 20M runs: {c2}\")\n",
    "print(f\"Cost of 150M runs: {c15}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "def compute_ei(model, likelihood, train_y, test_x, with_grad=False):\n",
    "    # EI = (\\mu - f(xbest)) * \\Phi(Z) + \\phi(Z). Z = (\\mu - f(xbest)) / \\sigma\n",
    "    model.eval()\n",
    "    likelihood.eval()\n",
    "\n",
    "    if with_grad:\n",
    "        m_out = model(test_x)\n",
    "        observed_pred = likelihood(m_out)\n",
    "    else:\n",
    "        with torch.no_grad():\n",
    "            m_out = model(test_x)\n",
    "            observed_pred = likelihood(m_out)\n",
    "    mean = observed_pred.mean\n",
    "\n",
    "    # Compute EI\n",
    "    y_best = train_y.min()\n",
    "    delta = y_best - mean\n",
    "    sigma = observed_pred.variance.sqrt()\n",
    "    t1 = delta * torch.distributions.Normal(0, 1).cdf(delta / sigma)\n",
    "    t2 = sigma * torch.exp(torch.distributions.Normal(0, 1).log_prob(delta / sigma))\n",
    "    ei = t1 + t2\n",
    "    return ei\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Starting points: [[3.82926912e-01 2.31461603e-01 2.59254894e-01 2.74900514e-02\n",
      "  9.88665399e-02 1.50000000e+01 5.90000000e+01]]\n",
      "Initia EI: -0.004804460251097709\n",
      "[ 1.12176733  0.59067136  0.70754594 -0.69714324 -0.72284139 15.\n",
      " 56.82537527]\n",
      "-0.04273536578538418\n"
     ]
    }
   ],
   "source": [
    "from scipy.optimize import minimize\n",
    "\n",
    "x0 = get_random_points(1)\n",
    "x0 = torch.tensor(x0, dtype=torch.float64)\n",
    "\n",
    "def ei_to_minimize(x):\n",
    "    x = torch.tensor(x).reshape(1, -1)\n",
    "    return -compute_ei(model, likelihood, train_y, x).detach().numpy().item()\n",
    "\n",
    "def ei_grad(x):\n",
    "    x = torch.tensor(x, dtype=torch.double).reshape(1, -1)\n",
    "    x.requires_grad = True\n",
    "    ei = -compute_ei(model, likelihood, train_y, x, with_grad=True)\n",
    "    ei.backward()\n",
    "    return x.grad.numpy()\n",
    "\n",
    "\n",
    "x0 = get_random_points(1)\n",
    "print(f\"Starting points: {x0}\")\n",
    "print(f\"Initia EI: {ei_to_minimize(x0)}\")\n",
    "\n",
    "res = minimize(ei_to_minimize, x0=x0.squeeze(0), bounds=[(None, None)] * 5 + [(2, 100)] + [(1, 197)], method='L-BFGS-B', jac=ei_grad)\n",
    "print(res.x)\n",
    "print(res.fun)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Revealing labels:   0%|          | 0/2 [00:00<?, ?it/s]/var/folders/t3/ps6yd2g900v_jnbbvn4cq_wc0000gn/T/ipykernel_27150/954095651.py:50: RuntimeWarning: Method L-BFGS-B cannot handle constraints.\n",
      "  minimize(\n",
      "/Users/michi/miniconda3/envs/dr/lib/python3.9/site-packages/gpytorch/likelihoods/gaussian_likelihood.py:347: GPInputWarning: You have passed data through a FixedNoiseGaussianLikelihood that did not match the size of the fixed noise, *and* you did not specify noise. This is treated as a no-op.\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Remaining results 10\n",
      "Constraint: 0.0. Success: True\n",
      "Remaining results 10\n",
      "Constraint: -2.220446049250313e-16. Success: True\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": []
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Remaining results 10\n",
      "Constraint: 0.0. Success: True\n",
      "Remaining results 0\n"
     ]
    }
   ],
   "source": [
    "import warnings\n",
    "from linear_operator.utils.warnings import NumericalWarning\n",
    "from collections import defaultdict\n",
    "warnings.filterwarnings('ignore', category=NumericalWarning)\n",
    "\n",
    "# TODO: Limit search space to within each scale.\n",
    "# TODO: Take gradient over step as well but discretize at the end\n",
    "from tqdm import tqdm\n",
    "from scipy.optimize import minimize\n",
    "\n",
    "num_revealed = 2\n",
    "num_search_per_fid = 10\n",
    "\n",
    "for i in tqdm(range(num_revealed), desc=\"Revealing labels\"):\n",
    "    # Find the point with the highest EI\n",
    "    def ei_to_minimize(x, scale, timestep):\n",
    "        x = torch.tensor(x, dtype=torch.double).reshape(1, -1)\n",
    "        x = torch.exp(x)\n",
    "        x = x / torch.sum(x)\n",
    "        x = torch.cat([x, torch.tensor([scale, timestep], dtype=torch.double).reshape(1, -1)], dim=1)\n",
    "\n",
    "        return -compute_ei(model, likelihood, train_y, x).detach().numpy().item()\n",
    "\n",
    "    def ei_grad(x, scale, timestep):\n",
    "        x = torch.tensor(x, dtype=torch.double).reshape(1, -1)\n",
    "        x = torch.exp(x)\n",
    "        x = x / torch.sum(x)\n",
    "        x = torch.cat([x, torch.tensor([scale, timestep], dtype=torch.double).reshape(1, -1)], dim=1)\n",
    "\n",
    "        x.requires_grad = True\n",
    "        ei = -compute_ei(model, likelihood, train_y, x, with_grad=True)\n",
    "        ei.backward()\n",
    "        grad = x.grad[0, :5].squeeze(0).numpy()\n",
    "\n",
    "        return grad\n",
    "\n",
    "    ei_results = defaultdict(defaultdict)\n",
    "    sampled_points = []\n",
    "    for scale in tqdm([2, 6, 15, 30, 50, 70, 100], desc=\"Scales\", position=1, leave=False):\n",
    "        for timestep in [60, 120, 197]:\n",
    "            x0s = get_random_points(num_search_per_fid)\n",
    "\n",
    "            # Bound x to sum to 1\n",
    "            def sum_constraint(x):\n",
    "                return np.sum(x) - 1  # Will equal 0 when sum is 1\n",
    "            constraints = ({'type': 'eq', 'fun': sum_constraint})\n",
    "            results = list(\n",
    "                map(\n",
    "                    lambda x0:\n",
    "                    minimize(\n",
    "                        lambda x: ei_to_minimize(x, scale, timestep), x0=x0[:5], bounds=[(0, 1)] * 5, method='L-BFGS-B', jac=lambda x: ei_grad(x, scale, timestep),\n",
    "                        constraints=constraints\n",
    "                    ), x0s\n",
    "                )\n",
    "            )\n",
    "\n",
    "            # Filter results not successful and not satisfying constraints\n",
    "            results = list(filter(lambda res: res.success and np.isclose(sum_constraint(res.x), 0.0), results))\n",
    "            print(f\"Remaining results {len(results)}\")\n",
    "            if len(results) == 0:\n",
    "                print(f\"Scale {scale}, timestep {timestep} has no successful optimizing results\")\n",
    "                continue\n",
    "\n",
    "            best_result = max(results, key=lambda x: -x.fun)\n",
    "            cur_x = best_result.x\n",
    "            print(f\"Constraint: {sum_constraint(cur_x)}. Success: {best_result.success}\")\n",
    "            ei_results[scale][timestep] = (cur_x, -best_result.fun)\n",
    "\n",
    "    scaled_ei_results = defaultdict(defaultdict)\n",
    "    max_scaled_ei = -1\n",
    "\n",
    "    for scale in ei_results.keys():\n",
    "        for timestep in ei_results[scale].keys():\n",
    "            scaled_ei_results[scale][timestep] = (\n",
    "                ei_results[scale][timestep][0],\n",
    "                ei_results[scale][timestep][1] / flops[scale] / timestep\n",
    "            )\n",
    "            scaled_ei = scaled_ei_results[scale][timestep][1]\n",
    "            if scaled_ei > max_scaled_ei:\n",
    "                max_scaled_ei = scaled_ei\n",
    "\n",
    "                best_scale = scale\n",
    "                best_timestep = timestep\n",
    "                best_x = ei_results[scale][timestep][0]\n",
    "\n",
    "    print(scaled_ei_results)\n",
    "    print(f\"Best ei: {max_scaled_ei},\\n\\tscale: {best_scale},\\n\\ttimestep: {best_timestep},\\n\\tx: {best_x}\")\n",
    "    sampled_points.append((best_scale, best_timestep, best_x))\n",
    "\n",
    "    # Reveal the values of the point with the highest EI\n",
    "    # cur_x = best_result.x\n",
    "    # print(f\"Revealing {cur_x}\")\n",
    "    # cur_y = func(z, cur_x)\n",
    "    # chosen_x.append(cur_x.item())\n",
    "    # opt_ys.append(cur_y.item())\n",
    "\n",
    "    # Update the model with the new point\n",
    "    new_x = torch.tensor([np.concatenate((best_x, np.array([best_scale, t]))) for t in range(1, best_timestep + 1)])\n",
    "    new_y = torch.tensor([func(t, best_scale, best_x) for t in range(1, best_timestep + 1)])\n",
    "    train_x = torch.cat([train_x, new_x], dim=0)\n",
    "    train_y = torch.cat([train_y, new_y], dim=0)\n",
    "\n",
    "    model.set_train_data(train_x, train_y, strict=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[(6, 60, array([1., 0., 0., 0., 0.]))]"
      ]
     },
     "execution_count": 27,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "sampled_points"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "True"
      ]
     },
     "execution_count": 36,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "np.isclose(1, 1.00001)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "dr",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.19"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
